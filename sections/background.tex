
\begin{table*}[tp]
    \caption{Overview of Dynamic Miss Minimizing Cache Partitioning Techniques}
    \label{tab:cachePartTaxonomy}
    \begin{tabularx}{\textwidth}{lcXXXX}
    \toprule
    Year & Citations & Scheme & Feedback Mechanism & Partitioning Mechanism \\    
    \midrule
    *2002 & & Suh et al.\ \cite{suh02,dynPartofSharedCacheMemory} & Marginal gain counters & Cache quota partitioning \\    
    2006 & & Utility Based Cache Partitioning (UCP) \cite{utilityBasedCachePartitioning} & Auxiliary Tag Directory (ATD) & Way Partitioning \\ 
    *2006 & & Dybdahl et al.\ \cite{haakonHiPC} & ATD (called shadow tags) & Way partitioning \\
    2007 & & Thread-Aware Dynamic Insertion Policy (TADIP) \cite{jaleel08} & Set dueling \cite{shadowTagInsertionPolicies} & TADIP \\
    2009 & & Promotion/Insertion Pseudo-Partitioning (PIPP) \cite{xie09} & ATD & PIPP \\
    2010 & & Re-Reference Interval Prediction (RRIP) \cite{jaleel10} & Set Dueling (for DRRIP and TA-DRRIP) & SRRIP, DRRIP and TA-DRRIP replacement policies \\
    *2010 & & Xie et al.\ \cite{xie10} & ATD (called shadow tags) & Thrasher caging (way partitioning) \\
    2011 & & NUcache \cite{manikantan11} & DeliTrack and NUTrack & NUcache \\
    *2011 & & Vantage \cite{sanchez11} & ATD & Vantage \\
    2012 & & Cooperative Partitioning \cite{sundararajan12} & ATD & Column caching variant for energy reduction \\ % Power-directed global miss minimization
    2012 & & PriSM \cite{manikantan12} & ATD and counters & PriSM \\ % Global miss minimization, fairness and QoS 
    *2012 & & Duong et al.\ \cite{duong12} & Reuse distance distributions & Protecting Distance replacement \\
    *2012 & & Hasenplaugh et al.\ \cite{hasenplaugh12}  & Gradient regions & Adaptive insertion policy \\ % Global miss minimization, IPC and QoS
    2013 & &  Albericio et al.\ \cite{albericio13}  & Reused bits and beeing-used bits & Least Recently Reused and Not Recently Reused \\ % Global miss minimization (inclusive caches)
    \bottomrule
    \end{tabularx}    
\end{table*}


\toni{Add the missing works to Table \ref{tab:cachePartTaxonomy}}

\toni{Can you retrieve the number of citations of each global miss minimizing work from Google Scholar?
The older techniques we have implemented are likely cited a lot and then we can argue for why we selected the less cited works.}

\runar{In this section, we should discuss only the works from \ref{tab:cachePartTaxonomy}. Can you update the following sections accordingly and identify any missing discussions?}
\runar{Magnus: I added stars next to the algorithms missing in the table, and removed the discussion of LRU, DIP and CLU.}

\magnus{Write a paragraph justifying why we chose the five partitioning techniques that we did out of the at least 14 possibilities in the new Table \ref{tab:cachePartTaxonomy}}

\magnus{When the classification is complete, group works according to explicit/implicit and optimize the table wrt area}

\subsection{Implicit Cache Partitioning Techniques}

\subsubsection{TADIP}
Thread-Aware DIP (TADIP)~\cite{Jaleel2008} was first proposed in 2008.
TADIP organizes the cache set as a stack, simular to LRU.
Like in LRU, blocks that are accessed are promoted to the top of the stack and the block at the bottom of the stack is selected for replacement.
A new insertion policy Binomial Insertion Policy (BIP) is introduced in addition to the well known LRU Insertion Policy (LIP).
LIP inserts new blocks the op of the stack (MRU position).
BIP inserts new blocks mostly at the bottom of the stack (LRU position), or with a small probability, $p = \frac{1}{32}$, at the MRU position.
TADIP dynamically switch between LIP and BIP on a per-core basis, based on performance measurements using duel-sets.

By mostly inserting at the LRU position the BIP insertion policy can handle trashing memory access patterns.
When most new blocks enter at the LRU position, the upper parts of the LRU stack can contain blocks that have been re-referenced.
In a trashing access pattern, this results in part of the working set residing in the upper part of the stack while the rest are inserted at the LRU position and evicted at the next cache miss.
By sometimes inserting at the MRU position, BIP will allow some blocks a higher chance of being re-referenced, while also forcing stale cache blocks towards the LRU position. 

The set-dueling architecture used to detect the best performer of the two algorithms consists of two distinct duel-set per core.
Each duel-set statically utilizes either LIP or BIP for the given core.
All other sets, follower sets, dynamically switch between LIP or BIP depending on the performance of the duel-sets.
When a core accesses another core's duel-set, it behaves similar to when it accesses a follower set.
Because of this the duel-sets are measuring algorithm performance given the feedback from other cores running their currently best algorithm.
The authors call this implementation of the algorithm TADIP-Feedback.

\subsubsection{DRRIP}

Dynamic Re-Reference Interval Prediction (DRRIP)~\cite{Jaleel2010} was proposed in 2010.
In DRRIP, each cache block has a number associated with it, called re-reference interval.
The re-reference interval is a relative measure of when the algorithm expects a block to be re-referenced.
Given two blocks with different re-reference intervals, then the block with a lower interval is expected to be re-referenced before the other block.
A value between 0 and $2^M - 1$ is used to represent the re-reference interval.
M is a configurable variable usually in the interval $[2, 5]$~\cite{Jaleel2010}.
A re-reference interval of 0 indicates a \textit{near} re-reference, the algorithm expects the block to be re-referenced in the near future.
The value $2^M - 1$ indicates a \textit{distant} re-reference interval while the value of $2^M - 2$ indicates a \textit{long} re-reference interval.
Multiple blocks may have the same re-reference interval. 
Hence, blocks are not strictly ordered as in the LRU stack.

The replacement policy of DRRIP is to scan all blocks and evict the first one found with a distant re-reference interval.
If no blocks have a distant re-reference interval the re-reference interval of all blocks is incremented by one and the scan restarts.
This process repeats until the algorithm finds a victim block.
If multiple blocks are potential victims, the algorithm uses the scan order as a tie-breaker.

DRRIP's promotion policy is to decrement the re-reference interval of the accessed block.
By doing this DRRIP utilize access history rather than access time when calculating the re-reference interval.
Hence, to reach a near re-reference interval a block has to be accessed multiple times.
The insertion policy of DRRIP, like DIP and TADIP, is composed of two different policies and a selection mechanism.
Static RRIP (SRRIP) will always insert new blocks with a long re-reference interval. 
Depending on the state of the cache, there might be existing blocks with a higher re-reference interval than the blocks inserted by SRRIP.
This gives the newly inserted blocks a chance to see a re-reference before being replaced.
Binomial RRIP (BRRIP) with either insert new blocks with a distant re-reference interval or, with a small probability, insert like SRRIP with a long re-reference interval.
Like BIP, BRRIP will allow trashing access patterns to keep some of the working set in the cache and hence improve performance over SRRIP.
The authors suggest using set dueling or ATDs to select between the two algorithms, similar to what was describe for TADIP.
Selecting between the two insertion policies can be done using set dueling or ATDs, similar to what was described for TADIP.

\subsubsection{NUCache}

NUCache~\cite{Manikantan2011} was first proposed in 2011.
NUCache is based on the concept of delinquent PCs.
A delinquent PC is the PC value of a memory instruction that often causes cache misses.
By evaluating the properties of the delinquent PCs, NUCache selects a set of PCs and allocates more cache space to blocks loaded by these instructions.
Because all applications running may contain one or more delinquent PCs, NUCache will implicitly share the cache between the applications.

NUCache monitors cache accesses from all instructions and uses statistics to select a set of PCs that both cause frequent cache misses and show reuse by referencing the same data repeatedly.
The algorithm divides the ways in each cache set into two groups; MainWays, and DeliWays.
The MainWays are managed by LRU while the DeliWays are simply first in first out.
The value $M$ defines the number of DeliWays.
NUCache attempts to reduce misses by not evicting blocks from selected delinquent PCs when they are evicted from the MainWays, but rather let them enter the DeliWays.
By using the value of $M$ and the set of delinquent PCs detected, the algorithm periodically selects a set of PCs that are allowed to use the DeliWays.
The selection is done using a greedy algorithm that attempts to ensure that each block entering the DeliWays will receive a hit at least once before they are pushed out by other blocks.
DeliWays and MainWays are implemented by having two extra bits per cache block, one indicating if the block can enter the DeliWays, another indicating if the block is the DeliWays.
On insertion, all blocks inserted into to the MainWays.
When the LRU block in the MainWays is about to be replaced, the algorithm checks if it is marked to enter the DeliWays.
If the block is allowed to enter the DeliWays, it will not be evicted but rather moved from the MainWays.
If, after moving the new block into the DeliWays, the number of DeliWays blocks has exceeded $M$ the oldest block is removed.
Otherwise, the new LRU block in the MainWays is evaluated.
Because of this implementation the MainWays may use the entire cache if no DeliWays are in use, at the same time the DeliWays cannot exceed $M$.
This allows for an efficient use of every cache set.

\subsubsection{Reuse Cache}

The Reuse Cache was first introduced in 2013.
It is an inclusive shared cache in which eviction order depends on how valuable lines are: lines neither present in the private caches nor showing reuse are considered the least valuable and are evicted first; next, lines not present in the private caches but reused are evicted; and, finally, the most valuable lines, those being stored in the private caches, are evicted only when lines of no other types are stored in the set, which should happen rarely.

A \textit{being-used} bit is used to record whether a line is present in any private caches when this information cannot be deduced from a directory. 
In addition, a line is considered reused when there was at least one hit on it after it was loaded.

Two novel replacement policies implement the process described above: least recently reused (LRR) and not recently reused (NRR), inspired by LRU and NRU, respectively. 

LRR orders the lines of each set in a least recently reused stack depending on their last hit. 
There is no relative order among non-reused lines, and being-used lines are considered unordered too. 
Thus, random replacement is employed when a line from these groups has to be evicted. 
On the other hand, reused lines are victimized according to their reuse order. 

To reduce implementation costs of the LRR stack, NRR adapts the NRU algorithm for tracking reuse, requiring a single NRR bit per line to distinguish the recently reused lines from the not recently reused ones.
In contrast with NRU, the reuse bit will be unset only on hits, while it will be set on line-fills. 
Lines with their NRR bit set are considered non-reused and victimized first.

When multiple processes compete for the LLC, both NRR and NRU implicitly evict first those memory blocks coming from streaming applications, as they show no reuse. 
These policies also give those processes heavily reusing their lines more shared cache capacity.
Finally, all lines stored in the private caches of all cores are kept in the shared LLC, which may not be the case in other partition policies.

\subsection{Explicit Cache Partitioning Techniques}

\subsubsection{UCP}

Utility Cache Partition (UCP)~\cite{Qureshi2006} was first presented in 2006. 
UCP uses the concept of utility when assigning ways to a core.
Using a Utility Monitor (UMON), UCP divides the ways in the cache between the cores.
UCP then uses the same insertion and promotion policy as LRU.
The replacement policy is as in LRU but with two modifications:
First if the number of blocks owned by the requesting core is less than the number of ways assigned to it, then the least recently used block that is not assigned to the requester core is replaced.
If however the number of blocks owned is greater than or equal to the number of assigned ways the replacement algorithm selects the least recently used block of those owned by the requester.
This replacement policy ensures that the division between cores in each set move toward the global allocation following cache misses.
At the same time, a core may use more blocks that it is currently assigned, given that the space is not claimed by any other core.

The UMON is the core of the UCP algorithm.
It consists of one ATD per core sharing the cache. 
The ATDs uses the stack property of LRU and counts the number of hits in all valid partition sizes using one hit counter per cache way.
Based on these hit counters a greedy algorithm divides the cache ways between all cores in an attempt to reduce the overall number of misses.

\subsubsection{PIPP}

Promotion/Insertion Pseudo-Partitioning (PIPP)~\cite{Xie2009} proposed in 2009 is an algorithm based on a slightly modified UMON circuit and a novel insertion and promotion policy.
The UMON used by PIPP is extended with stream detection support.
This allows the algorithm to enforce special policies to cores that are experiencing no or very few re-references.
An application is said to be streaming if the total number of misses in the ATD is above a set limit, or if the miss-hit ratio is above a threshold.

PIPP treat the cache set as an LRU stack, and the normal LRU replacement policy is used.
The algorithm introduces a novel insertion and promotion policy.
The insertion policy inserts new blocks $\pi_n$ blocks from the LRU position. 
Here $\pi_n$ is the number of ways assigned to the $n^{th}$ core.
On a cache access, a block has a chance, $p_{prom} = \frac{3}{4}$, to move one position upwards in the stack unless it is already at the MRU position.

On insertion, the PIPP policy does not consider how many blocks are owned by the requesting core, this is unlike UCP's insertion policy that prevents a core from claiming more ways that what it is assigned.
However, cores with more ways assigned to it will insert its blocks higher up in the stack. 
The core with the highest number of ways assigned will not have any insertion competition pushing its blocks out of the cache.
The only way blocks from this core can be pushed out is by other blocks from the same core, or by blocks from other cores that are re-referenced repeatably.
Two cores with the same allocations will both have an equal chance of keeping their blocks in the cache, as they both insert at the same position.
Statistically a core with a lower allocation, inserting at a lower position in the stack, should also on average own fewer blocks in the cache compared to a core with a higher allocation.
This way PIPP obtains what the original authors call pseudo partitioning, where overall a higher allocation will statistically result in more cache space.
However, the access frequency of cores can cause a core with a low allocation to own most of or all blocks in the cache if the other cores have a much lower access frequency.

When the UMON detects a core that is streaming PIPP will no longer insert blocks from this core at the position given by the allocation.
A special insertion position, $\pi_{stream}$, is used for all streaming cores.
$\pi_{stream}$ is set to the number of cores currently streaming. 
By inserting at this fixed position, PIPP attempts to limit the interference the streaming core has on the non-streaming cores.
Blocks from streaming applications have a reduced chance of promotion after an access, $p_{stream} = \frac{1}{128}$.
In the case where all cores are streaming, and there are no cores to protect, PIPP inserts all blocks at the LRU position.

A potential issue with PIPP which may cause performance degregation in some cases have been documented by previous works~\cite{Manikantan2012}.
The observation is that blocks are replaced too quicly, before seeing any real re-use.
On proposed solution is to add a constant to the insertion offset $\pi_n$.
In additon to PIPP, we have implemented PIPP-min8 where we add the constant value 8 to any insertion position.

\subsubsection{PriSM}

PriSM~\cite{Manikantan2012} was first presented in 2012.
PriSM is a framework for cache management with optimization algorithms targetting multiple performance goals.
The original paper presents hit maximization, fairness, and QoS goals.
PriSM utilizes ATDs to estimate private cache performance for each of the cores.
The ATD will only keep track of total number misses and hits.
In addition to the ATDs, the algorithm requires three counters per core tracking hits, misses and number of blocks owned by the core in the actual cache.
PriSM utilizes the same insertion and promotion policies as LRU, but the replacement policy is optimized based on the ATD and the optimization target.

The replacement algorithm of PriSM utilizes eviction probabilities, $E_i$ ($\sum{E_i} = 1$), assigned to each core when selecting a victim block.
On replacement, a victim core is first selected by a random draw using the eviction probabilities.
The LRU block owned by the victim core within the cache set is the eviction target.
In the case where the selected target does not own a block in the set, all blocks owned by cores with $E_i > 0$ are considered, and the LRU of these is the eviction target.
At set intervals, an optimization algorithm determines the eviction probability, $E_i$, for each core.
The original paper recalculated $E_i$ values at every 10000 cache miss.

Selecting an eviction probability $E_i$ for each core is done by considering how the eviction probability will effect a core's usage of the cache.
Consider an interval of W misses where each core contributes a fraction of the misses, $M_i$.
At the start of the interval the blocks owned by $core_i$ equals a fraction $C_i$ of the total number of blocks in the cache.
If we do not evict any blocks owned by $core_i$ during the interval, then at the end of the interval the core owns a fraction $T_i$ of the cache.
$T_i$ is known as the target allocation, and is expressed by $T_i = C_i + M_i * W/N$. 
Here $M_i * W$ is the number of misses caused by $core_i$ during the interval, which also is the number of blocks inserted by the core.
$N$ is the total number of blocks in the cache, and the fraction $M_i * W/N$ equals the fraction of the cache claimed by $core_i$ during the interval.
If the core has a non-zero eviction probability, then this formula extends into $T_i = C_i + (M_i - E_i) * W/N$.
The above formula can be rearranged to provide a eviction probability given target allocation: $E_i = (C_i - T_i) * N/W + M_i$.
As noted, PriSM defines three optimization targets, each one of these is responsible for calculating the optimal $T_i$ that will fulfill the optimization target given the data available in the ATDs and per core counters.

\subsubsection{CCP}

Cooperative Cache Partitioning (CCP)~\cite{cooperativeCachePartitioning} was first proposed in 2007. 
Rather than partitioning the cache spatially like in most other partitioning mechanisms, CPP uses Multiple Time-sharing Partitions (MTP) to speed-up one thrashing thread by temporarily shrinking the capacity of other threads. 
Threads are classified in three categories: (1) \textit{supplier threads} that can supply some or all of their equal-share capacity with no performance loss, (2) \textit{sensitive threads} whose performance changes significantly as cache size varies in the proximity of their equal-share capacity and (3) \textit{thrashing threads} whose working-set is much larger than their equal-share capacity.
An expanded partition can provide a large speed-up to a thrashing thread when its working set fits in the expanded partition, easily compensating for tiny slowdowns caused by shrinking partitions. 

CCP defines guaranteed partitions as \textit{the minimum amount of cache space required to achieve the same level of performance as using the equal-share cache allocation}, and C$_{expand}$ and C$_{shrink}$ as the capacities used by a thread in its expanding and shrinking partitions, respectively. 

CCP measures each thread's L2 cache miss rates under candidate cache allocations (e.g., with UMON), and uses them to estimate the IPC curve, which is used to calculate each thread's guaranteed partition. 
Supplier threads are allocated their (small) guaranteed partitions.
The remaining space is allocated among other threads.
For this, the CCP partition algorithm calculates C$_{expand}$ and C$_{shrink}$, and returns a set of MTP partitions using a \textit{thrashing thread test} as a simple heuristic.
Each of these partitions benefits one thrashing thread by starving other thrashing threads with their C$_{shrink}$ capacity, and if one expanding thread can not use all the remaining space, other threads are expanded to further increase speedup. 

CPP finally combines MTP with Cooperative Caching (CC)~\cite{chang06} by dividing execution time into epochs managed by either MTP or CC, weighted by the number of threads that benefit from each of them.
