\section{Introduction}

Most Chip Multiprocessors (CMPs) share parts of the memory system to improve resource utilization.
In particular, the last level cache (LLC) is commonly shared between processor cores.
This design choice makes destructive cache interference possible which reduces performance and may cause problems such as missed deadlines, priority inversion, unpredictable interactive performance and non-compliance with service level agreements \cite{dubois13}.
For this reason, computer architecture researchers have proposed a large number of microarchitectural techniques that partition the LLC between competing processes \magnus{add long list of citations}.
Given the large amount of cache partitioning research, it is common to reuse previously proposed ideas in new schemes.
Unfortunately, it can be hard to identify this reuse without understanding the work in detail.
In this work, we systematically analyze the cache partitioning research field and identify overlap with the aim of helping future architects avoid reinventing the wheel.

We loosely define the cache partitioning problem as the task of managing shared LLC capacity in a thread-aware manner with the aim of achieving a performance-related goal.
We differentiate between \textit{explicit} and \textit{implicit} cache partitioning.
In explicit cache partitioning, each running process is assigned an LLC quota which determines the minimum amount of cache space the process will have \magnus{add references to examples}.
With implicit partitioning, LLC space is managed by process-aware LLC policies without assigning a specific quota to each process \magnus{add references to examples}.

The first contribution of this work is a classification of previously proposed cache partitioning techniques.
All cache partitioning techniques contain a \textit{policy} which defines the goal to be achieved with partitioning and \textit{mechanisms} that are the primitives used to implement the policy \cite{virtualPrivateMachines}.
The two most common policies are to minimize the total number of LLC misses or to provide \textit{Quality of Service (QoS)} by ensuring that the performance of one or more processes is kept above a certain level.
Cache partitioning techniques also commonly use a \textit{feedback} mechanism and a \textit{partitioning} mechanism.
\magnus{Will splitting the partitioning mechanism into partitioning and enforcement simplify the arguments?}
The feedback mechanism gathers information regarding the cache requirements of the currently running processes while the partitioning mechanism is used to select and enforce partitions (explicitly or implicitly).
Examples of feedback mechanisms are \textit{Auxilliary Tag Directories (ATDs)} \cite{utilityBasedCachePartitioning,haakonHiPC} and marginal gain counters \cite{dynPartofSharedCacheMemory,suh02} and examples of partitioning mechanisms are way partitioning  \cite{utilityBasedCachePartitioning} and modified promotion and insertion policies (e.g.\ \cite{xie09}).

The main observation resulting from this classification is that almost all proposed cache partitioning techniques apply previously proposed ideas in some form.
For instance, the ATD feedback mechanism has been used extensively \magnus{add complete list of citations}.
This is not surprising given the large number of proposed techniques targeting cache partitioning, but for science to progress it is critical that these overlaps are clearly identified.
This work is to the best of our knowledge the first to systematically classify the policies and mechanisms used in previous cache partitioning schemes.
Thus, our classification scheme enable architects to clearly separate their novel ideas from previous work.

The second contribution of this work is a quantitative instigation of the performance of 5 high-impact cache partitioning techniques that collectively cover the current design space of miss minimizing partitioning techniques targeted at monolithic LLCs \magnus{verify that this is correct}.
Concretely, we implemented TADIP \cite{jaleel08}, DRRIP \cite{jaleel10}, UCP \cite{utilityBasedCachePartitioning}, PIPP \cite{xie09} and PriSM \cite{manikantan12}.
The baseline is a conventional LRU-managed shared LLC.
\magnus{What did we find?}
\magnus{Add the progression over time plot here?}
We aim to provide our implementations to the research community.

\magnus{Add outline of the work? Do we need it?}

% Previous "meta analyses", can we get them in somewhere: \cite{microlib,comparingPrevalingSimulationTechniques,desmet10}).

\section{Cache Partitioning Techniques}

\begin{table*}[tp]
	\caption{Cache Partitioning Technique Overview}
	\label{tab:cachePartTaxonomy}
	\begin{tabularx}{\textwidth}{llcXXX}
	\toprule
	Year & Scheme & Ref. & Policy & Feedback Mechanism & Partitioning Mechanism \\	
	\midrule
	2000 & Column Caching & \cite{columnCaching} & - & - & Column caching (first) \\
	2002 & Suh et al. & \cite{suh02,dynPartofSharedCacheMemory} & Global miss minimization & Marginal gain counters (first) & Cache quota partitioning (first?) \\
	2004 & Kim et al. & \cite{fairCacheSharingAndPartitioning} & Fairness & Marginal gain counters & Cache quota partitioning LRU \\
	2004 & CQoS & \cite{cqos} & Priority-based & - & Set partitioning, selective cache allocation and heterogeneous cache regions \\
	2006 & Utility Based Cache Partitioning (UCP) & \cite{utilityBasedCachePartitioning} & Global miss minimization & Auxiliary Tag Directory (ATD) & Way Partitioning \\ 
	2006 & Dybdahl et al. & \cite{haakonHiPC} & Global miss minimization & ATD (called shadow tags) & Way partitioning \\
	2006 & Settle et al. & \cite{dynReconfCache} & Reuse-based heuristics & Access and reuse counters & Column caching and LRU heuristic \\
	2006 & Rafique et al. & \cite{osSupportForFairCacheSharing} & OS-based priorities and miss-rate equalization \cite{fairCacheSharingAndPartitioning} & Marginal gain counters and IPC & Way partitioning \\
	2006 & Hsu et al. & \cite{sharedCachePolicies} & Equal share (Communist) and maximum total benefit (Utilitarian) & - & - \\
	2006 & STATSHARE & \cite{petoumenos06} & OS: Minimize miss rate, maximize "usefulness", QoS  & Sampled, quantized reuse-distance histograms  & Replace decayed over non-decayed cache lines randomly\\
	2007 & Cooperative Cache Partitioning (CCP) & \cite{cooperativeCachePartitioning} & Fair speedup and weighted speedup & - & Time-sharing \\
	2007 & Guo et al. & \cite{qosInCMPs} & QoS with admission policy, resource stealing and speculative downgrade  & ATD & Way partitioning \\
	2007 & Virtual Private Caches (VPC) & \cite{virtualPrivateCaches} & QoS with IPC targets, fairness policy distributes left-over resources & - & VPC Arbiter (based on network fair queuing) and VPC Capacity Manager (way partitioning) \\
	2007 & Thread-Aware Dynamic Insertion Policy (TADIP) & \cite{jaleel08} & Global miss rate minimization & Set dueling \cite{shadowTagInsertionPolicies} & TADIP \\
	2007 & Iyer et al. & \cite{qosPoliciesForCMP} & QoS: Static vs.\ dynamic & Shared unit utilization & Cache quota partitioning per QoS-level \\
	2009 & FlexDCP & \cite{flexDCP} & OS-guided performance fairness & ATD & Column caching \\
	2009 & Zhou et al. & \cite{zhou09} & Performance fairness (identical slowdowns) & Overlap counters and ATD & Overalloc counter and modified LRU \\
	2009 & Promotion/Insertion Pseudo-Partitioning (PIPP) & \cite{xie09} & Global miss minimization & ATD & PIPP \\
	2009 & SHARP Control & \cite{sharpControl} & Fair speedup and QoS & IPC target and miss rate models are input & Way partitioning \\
	2010 & Re-Reference Interval Prediction (RRIP) & \cite{jaleel10} & Global miss minimization & Set Dueling (for DRRIP and TA-DRRIP) & SRRIP, DRRIP and TA-DRRIP replacement policies \\
	2010 & Yu et al. & \cite{yu10} & Off-chip bandwidth minimization & Off-line profiling & Set partitioning \cite{ranganathan00}  \\
	2010 & Xie et al. & \cite{xie10} & \toni{Fill out if needed} & & \\
	2011 & Wang et al. & \cite{wang11} & Meet deadlines and minimize energy & Off-line profiling & Static way partitioning \\
	2011 & NUcache & \cite{manikantan11} & Minimize global miss rate & DeliTrack and NUTrack & NUcache \\
	2011 & Vantage & \cite{sanchez11} & Global miss minimization & ATD & Vantage \\
	2012 & Cooperative Partitioning & \cite{sundararajan12} & Power-directed global miss minimization & ATD & Column caching variant for energy reduction \\
	2012 & PriSM & \cite{manikantan12} & Global miss minimization, fairness and QoS & ATD and counters & PriSM \\
	2012 & Duong et al. & \cite{duong12} & Global miss minimization & Reuse distance distributions &Protecting Distance replacement \\
	2012 & Hasenplaugh et al. & \cite{hasenplaugh12} & Global miss minimization, IPC and QoS  & Gradient regions & Adaptive insertion policy \\
	2013 & Albericio et al. & \cite{albericio13} & Global miss minimization (inclusive caches) & Reused bits and beeing-used bits & Least Recently Reused and Not Recently Reused \\
	\bottomrule
	\end{tabularx}	
\end{table*}

\textit{Note: Table \ref{tab:cachePartTaxonomy} is copy pasted from my previous manuscript and will have to be modified or possibly removed.}

\runar{Add a condensed version of your background chapter here}

\toni{Add the missing works after Runar has added his stuff}

\textit{We should retrieve the number of citations of each work from Google Scholar.
The older techniques we have implemented are likely cited a lot and then we can argue for why we selected the less cited works.}

\section{Methodology}

\runar{Add a condensed version of your methodology}

\section{Results}

\runar{Add your experiments, condensed}

\section{Related Work}

\toni{Dense discussion of works we did not cover earlier. (Do we even need this section?)}

\section{Conclusion}

\magnus{Write conclusion}